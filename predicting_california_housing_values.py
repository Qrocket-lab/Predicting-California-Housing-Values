# -*- coding: utf-8 -*-
"""Predicting California Housing Values

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lQV4eJCumBZcHIfHtGS_f24jL_oMH5Q-

# California housing dataset

#Overview

This Google Colab notebook explores the California housing dataset to predict median house values, primarily using linear regression. The analysis begins with data loading, exploration, and cleaning, including addressing missing values using k-Nearest Neighbors imputation. Exploratory data analysis (EDA) is performed through visualizations like histograms and correlation matrices, revealing skewed distributions and outliers in features like housing_median_age and median_house_value. The notebook investigates relationships between variables, notably identifying a strong positive correlation between median_income and median_house_value using both correlation analysis and the Chi-squared test. Finally, a simple linear regression model is constructed to quantify this relationship, showing that house values increase by approximately 41793.849 for each unit increase in the median income index, with a baseline house value of around 45085.5767 when the median income index is zero. The notebook also identifies the importance of median income when predicting housing values, based on the model coefficients.

##Objective

*  The aim is to model is to predict the median_house_value which is our target variable
*  Overcome missing data with a basic unsupervised learning data imputation

#Initial Assestment
"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import hashlib
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSnBNwKqsXR0MobM1LMPLZUyPCLeVhSl2diJ7dBpEqy4ZwokHf4KB9khUHRqJfetu08UMs0ebRZOoap/pub?gid=208180691&single=true&output=csv')

df.head()

df. info()

"""##Spot the missing values"""

# % of Missing Values
df.isnull().sum()/len(df)*100.0

# Let's show all columns with missing data as well:
df[df.isnull().any(axis=1)] # any missing data in columns

"""*  We can note that we have a number of features with some missing data, but not too many instances overall (257/20640);
*  Lets try an Unsupervised Learning (UL) approach based on the kNN model, we can use the function below and pass a dataframe to generated an imputed dataframe

##Unsupervised Learning (UL) desc

* Core Idea: The kNN algorithm, at its heart, relies on the concept of similarity or proximity. It assumes that data points that are close to each other in a feature space are likely to have similar characteristics.
* Imputation with kNN: When you have missing data (like the NaN values in your dataset), you can use kNN to "fill in" those blanks.

##Handling the missing data
"""

from sklearn.neighbors import KNeighborsRegressor

# function that imputes a dataframe
def impute_knn(df):

    ''' inputs: pandas df containing feature matrix '''
    ''' outputs: dataframe with NaN imputed '''
    # imputation with KNN unsupervised method

    # separate dataframe into numerical/categorical
    ldf = df.select_dtypes(include=[np.number])           # select numerical columns in df
    ldf_putaside = df.select_dtypes(exclude=[np.number])  # select categorical columns in df
    # define columns w/ and w/o missing data
    cols_nan = ldf.columns[ldf.isna().any()].tolist()         # columns w/ nan
    cols_no_nan = ldf.columns.difference(cols_nan).values     # columns w/o nan

    for col in cols_nan:
        imp_test = ldf[ldf[col].isna()]   # indicies which have missing data will become our test set
        imp_train = ldf.dropna()          # all indicies which have no missing data
        model = KNeighborsRegressor(n_neighbors=5)  # KNR Unsupervised Approach
        knr = model.fit(imp_train[cols_no_nan], imp_train[col])
        ldf.loc[df[col].isna(), col] = knr.predict(imp_test[cols_no_nan])

    return pd.concat([ldf,ldf_putaside],axis=1)

# Call function that imputes missing data
df2 = impute_knn(df)
# looks like we have a full feature matrix
df2.info()

"""#Exploratory Data Analysis

##Univariate Data

* Uni means "one." In statistics, univariate data means dealing with a dataset that focuses on a single variable at a time.
* I would like to see every variable in this dataset, to examine is there any relation with other variable
* Using histogram Chart to spot Data distribution ( certain models prefer less skewed distributions ), Outliers ( Low Noise Assumption can be detremental to model performance ), Odd patterns in data ( Data abnormalities also affect model performance ), Axis Scale ( Feature scale values can affect a models performance )
"""

numerical_df = df2.select_dtypes(include=['number'])

histogram_color = 'Green'
plt.figure(figsize=(15, 9), facecolor='black') # Set figure background to dark

for i, column in enumerate(numerical_df.columns):
    plt.subplot(3, 4, i + 1, facecolor='black') # Set subplot background to dark

    numerical_df[column].hist(bins=60, color=histogram_color, edgecolor='white') # Use white edgecolor for contrast

    plt.title(f'Histogram of {column}', color='white') # White title
    plt.xlabel(column, color='white') # White x-axis label
    plt.ylabel('Frequency', color='white') # White y-axis label
    plt.tick_params(axis='x', colors='white')    # White x-axis ticks
    plt.tick_params(axis='y', colors='white')    # White y-axis ticks
    plt.tight_layout()

plt.show()

numerical_df = df2.select_dtypes(include=['number'])

histogram_color = 'Green'
plt.figure(figsize=(15, 9))

for i, column in enumerate(numerical_df.columns):
    plt.subplot(3, 4, i + 1)

    numerical_df[column].hist(bins=60, color=histogram_color, edgecolor='black')

    plt.title(f'Histogram of {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.tight_layout()

plt.show()

"""###Odd Patterns & Outliers

* On first impression, a few outlier (inconsistent with entire set) groups are present in our data; possibly due to the way in which the data was sampled ( 'housing_median_age' & 'median_house_value' )
* House_median_age is one possible feature with such outliers. Also having a lot of local peaks (all are quite gradual) but one really odd peak at the maximum value stands out. It has some slight discontinuity in data (which becomes visible with the adjustment of bins)
* Feature Median_house_value has an odd peak at its maximum value (around 500k), which could be an outlier.

###Less Noticeable Outliers

* We have quite a few skewed (less centralised) data distributions, 6 features have such distributions, which is quite a lot and slightly concerning since we are going to use a relatively simple model.
* The range of the x axis for some of these features is quite broad (eg. population), indicating we have quite a few outliers, but unlike the first two, we can apply transformation to features and attempt to correct it.
* Population, total_bedrooms and total_rooms represent somewhat connected things, also have similar distribution which is skewed t owards smaller values.

## Descriptive Statistic
"""

def calculate_descriptive_stats(df):
    # Calculate descriptive statistics
    stats = df.describe()

df.describe()

"""## Correlation"""

spearman_corr_matrix = numerical_df.corr(method='spearman')
print(spearman_corr_matrix)

spearman_corr_matrix = numerical_df.corr(method='pearson')
print(spearman_corr_matrix)

def corrMat(df, method='pearson'):
    corr_mat = df.corr(method=method).round(2)
    f, ax = plt.subplots(figsize=(10, 8), facecolor='black')
    mask = np.zeros_like(corr_mat, dtype=bool)
    mask[np.triu_indices_from(mask)] = True
    sns.heatmap(corr_mat, mask=mask, vmin=-1, vmax=1, center=0,
                cmap='plasma', linewidths=1, linecolor='black', square=True, annot=True, cbar=True, ax=ax,
                annot_kws={'color': 'white'}, cbar_kws={'label': 'Correlation', 'ticks': [-1, -0.5, 0, 0.5, 1]}
                )
    ax.set_facecolor('black')
    ax.set_title(f'{method.capitalize()} Correlation Matrix', fontsize=16, color='white')
    ax.tick_params(axis='x', colors='white')  # Set x-axis tick colors to white
    ax.tick_params(axis='y', colors='white')  # Set y-axis tick colors to white
    cbar = ax.collections[0].colorbar
    cbar.ax.yaxis.set_tick_params(color='white')
    cbar.ax.yaxis.label.set_color('white')
    plt.tight_layout()
    plt.show()

# Assuming df2 is your DataFrame and numerical_df is defined
# corrMat(numerical_df) # For Pearson
# corrMat(numerical_df, method='spearman') # For Spearman

def corrMat(df,method='pearson'):

    corr_mat = df.corr(method=method).round(2)
    f, ax = plt.subplots(figsize=(10, 8), facecolor='black')  # Increased figure size for better readability
    mask = np.zeros_like(corr_mat, dtype=bool)
    mask[np.triu_indices_from(mask)] = True
    sns.heatmap(corr_mat, mask=mask, vmin=-1, vmax=1, center=0,
                cmap='plasma', linewidths=1, linecolor='black', square=True, annot=True, cbar=True, ax=ax,
                annot_kws={'color': 'white'}, cbar_kws={'label': 'Correlation', 'ticks': [-1, -0.5, 0, 0.5, 1]}
                )
    ax.set_facecolor('black')
    ax.set_title(f'{method.capitalize()} Correlation Matrix', fontsize=16, color='white')
    ax.tick_params(axis='x', colors='white')  # Set x-axis tick colors to white
    ax.tick_params(axis='y', colors='white')  # Set y-axis tick colors to white
    cbar = ax.collections[0].colorbar
    cbar.ax.yaxis.set_tick_params(color='white')
    cbar.ax.yaxis.label.set_color('white')
    plt.tight_layout()
    plt.show()
corrMat(numerical_df)

def corrMat(df, method='spearman'):
    corr_mat = df.corr(method=method).round(2)
    f, ax = plt.subplots(figsize=(10, 8), facecolor='black')  # Increased figure size for better readability
    mask = np.zeros_like(corr_mat, dtype=bool)
    mask[np.triu_indices_from(mask)] = True
    sns.heatmap(corr_mat, mask=mask, vmin=-1, vmax=1, center=0,
                cmap='plasma', linewidths=1, linecolor='black', square=True, annot=True, cbar=True, ax=ax,
                annot_kws={'color': 'white'}, cbar_kws={'label': 'Correlation', 'ticks': [-1, -0.5, 0, 0.5, 1]}
                )
    ax.set_facecolor('black')
    ax.set_title(f'{method.capitalize()} Correlation Matrix', fontsize=16, color='white')
    ax.tick_params(axis='x', colors='white')  # Set x-axis tick colors to white
    ax.tick_params(axis='y', colors='white')  # Set y-axis tick colors to white
    cbar = ax.collections[0].colorbar
    cbar.ax.yaxis.set_tick_params(color='white')
    cbar.ax.yaxis.label.set_color('white')
    plt.tight_layout()
    plt.show()


corrMat(numerical_df, method='spearman')

def corrMat(df,method='spearman'):

    corr_mat = df.corr(method=method).round(2)
    f, ax = plt.subplots(figsize=(10, 8))  # Increased figure size for better readability
    mask = np.zeros_like(corr_mat, dtype=bool)
    mask[np.triu_indices_from(mask)] = True
    sns.heatmap(corr_mat, mask=mask, vmin=-1, vmax=1, center=0,
                cmap='plasma', linewidths=1, linecolor='white', square=True, annot=True, cbar=True, ax=ax)  # Added linecolor
    ax.set_title(f'{method.capitalize()} Correlation Matrix', fontsize=16) # Add the title depending on the method
    plt.tight_layout()
    plt.show()

corrMat(numerical_df, method='spearman')



"""###Key Note from correlation

* The Data shows that our aim variable is a moderate positive correlation, median_income and median_house_value have a correlation of 0.69. Higher incomes tend to be associated with higher house values.

#Chi Squared
"""

from scipy.stats import chi2_contingency

df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSnBNwKqsXR0MobM1LMPLZUyPCLeVhSl2diJ7dBpEqy4ZwokHf4KB9khUHRqJfetu08UMs0ebRZOoap/pub?gid=208180691&single=true&output=csv')

numerical_df = df.select_dtypes(include=['number'])
income_categories = pd.cut(numerical_df['median_income'], bins=3, labels=['Low', 'Medium', 'High'])
house_value_categories = pd.cut(numerical_df['median_house_value'], bins=3, labels=['Low', 'Medium', 'High'])

contingency_table = pd.crosstab(income_categories, house_value_categories)

stat, p, dof, expected = chi2_contingency(contingency_table)

# Print the results
print("Chi-squared statistic:", stat)
print("P-value:", p)
print("Degrees of freedom:", dof)
print("Expected frequencies:\n", expected)

alpha = 0.05

if p <= alpha:
  print('Reject H0')
  print('There is no relationship between Median Income and Median House Value')
else:
  print('Accept H0')
  print('There is a relationship between Median Income and Median House Value')

"""##Key Note

* There is no relationship between Median Income and Median House Value

* This statement might be a little confusing at first, because it is saying that there is no relationship, while at the same time you are rejecting the hypothesis that there is no relationship. It should be rephrased.

* Based on these results, you have very strong evidence to conclude that there is a statistically significant relationship between median income and median house value in your California housing dataset.

* Therefore, we conclude that there is a statistically significant association between median income and median house value. In simpler terms, they are related.

## Important Considerations

* Statistical Significance vs. Practical Significance: While the results are statistically significant, always consider whether the relationship is practically important in the real world. In this case, a relationship between income and house value is expected and very relevant.
* Causation: The Chi-squared test only tells you if there is an association or relationship between the two variables. It does not prove that one variable causes the other. In this case, higher income is probably not causing houses to be more expensive, but they are related to each other.

#Linear Regression
"""

df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSnBNwKqsXR0MobM1LMPLZUyPCLeVhSl2diJ7dBpEqy4ZwokHf4KB9khUHRqJfetu08UMs0ebRZOoap/pub?gid=208180691&single=true&output=csv')
df.head()

from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

y = df['median_house_value']
X = df['median_income'].values.reshape(-1,1)

model = LinearRegression()

model.fit(X,y)

model.coef_

model.intercept_

# y=bX + e
#median_house_value = (41793.8492019 * median_income) + 45085.57670326799

"""#Conclusion

* In your California housing dataset, there's a positive linear relationship between median income and median house value. Specifically, the model predicts that for each unit increase in the median income index, the median house value will increase by approximately 41793.849, with a baseline median house value of around 45085.5767 when the median income index is zero.
"""

